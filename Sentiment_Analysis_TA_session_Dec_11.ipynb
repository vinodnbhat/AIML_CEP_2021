{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_TA_session_Dec_11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinodnbhat/AIML_CEP_2021/blob/main/Sentiment_Analysis_TA_session_Dec_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkadKlgrUuGJ"
      },
      "source": [
        "We will work with the IMDB dataset, which contains movie reviews from IMDB. Each review is labeled as 1 (for positive) or 0 (for negative) from the rating provided by users together with their reviews.\\\n",
        "This dataset is available [here](https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format/download).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_w006uRnD5r"
      },
      "source": [
        "Code referred from [here](https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch/notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbQpHge0ex8"
      },
      "source": [
        "#Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaFkKFqwvbWI",
        "outputId": "aac35c5a-a932-4088-93ab-549b7ce921e7"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur54Y7ZCik8h"
      },
      "source": [
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eY5gZ0ZvVxa8",
        "outputId": "29dd2800-f97c-480d-873a-7d211d43d069"
      },
      "source": [
        "## upload data files to this colab notebook and read them using the respective paths\n",
        "\n",
        "df_train = pd.read_csv('/content/Train.csv')\n",
        "df_val = pd.read_csv('/content/Valid.csv')\n",
        "df_test = pd.read_csv('/content/Test.csv')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I put this movie in my DVD player, and sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do people who do not know what a particula...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Even though I have great interest in Biblical ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  I grew up (b. 1965) watching and loving the Th...      0\n",
              "1  When I put this movie in my DVD player, and sa...      0\n",
              "2  Why do people who do not know what a particula...      0\n",
              "3  Even though I have great interest in Biblical ...      0\n",
              "4  Im a die hard Dads Army fan and nothing will e...      1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY49UuaC3TtP",
        "outputId": "f6c17e19-d639-44a6-a6e5-ef4ba26075d4"
      },
      "source": [
        "x_train, y_train = df_train['text'].values, df_train['label'].values\n",
        "x_val, y_val = df_val['text'].values, df_val['label'].values\n",
        "x_test, y_test = df_test['text'].values, df_test['label'].values\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of val data is {x_val.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (40000,)\n",
            "shape of val data is (5000,)\n",
            "shape of test data is (5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NpFY2q8w3hzx",
        "outputId": "2f878b49-d6c1-4273-d20a-a04561aece72"
      },
      "source": [
        "#plot of positive and negative class count in training set\n",
        "dd = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOP4nfU50i5L"
      },
      "source": [
        "#Pre-Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnwPqvM3tz0"
      },
      "source": [
        "def preprocess_string(s):\n",
        "  \"\"\" preprocessing string to remove special characters, white spaces and digits \"\"\"\n",
        "  s = re.sub(r\"[^\\w\\s]\", '', s) # Remove all non-word characters (everything except numbers and letters)\n",
        "  s = re.sub(r\"\\s+\", '', s) # Replace all runs of whitespaces with no space\n",
        "  s = re.sub(r\"\\d\", '', s) # replace digits with no space\n",
        "  return s\n",
        "\n",
        "def create_corpus(x_train):\n",
        "  \"\"\" creates dictionary of 1000 most frequent words in the training set and assigns token number to the words, returns dictionay (named corpus)\"\"\"\n",
        "  word_list = []\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  for sent in x_train:\n",
        "      for word in sent.lower().split():\n",
        "          word = preprocess_string(word)\n",
        "          if word not in stop_words and word != '':\n",
        "              word_list.append(word)\n",
        "\n",
        "  word_count = Counter(word_list)\n",
        "  # sorting on the basis of most common words\n",
        "  top_words = sorted(word_count, key=word_count.get, reverse=True)[:1000]\n",
        "  # creating a dict\n",
        "  corpus = {w:i+1 for i,w in enumerate(top_words)}\n",
        "  return corpus\n",
        "\n",
        "def preprocess(x, y, corpus):\n",
        "  \"\"\" encodes reviews according to created corpus dictionary\"\"\"\n",
        "  x_new = []\n",
        "  for sent in x:\n",
        "    x_new.append([corpus[preprocess_string(word)] for word in sent.lower().split() if preprocess_string(word) in corpus.keys()])\n",
        "\n",
        "  return np.array(x_new), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpXUwhgN7dBn",
        "outputId": "e0832b1c-02c6-424a-9413-bb03b01739b2"
      },
      "source": [
        "corpus = create_corpus(x_train)\n",
        "print(f'Length of vocabulary is {len(corpus)}')\n",
        "x_train, y_train = preprocess(x_train, y_train, corpus)\n",
        "x_val, y_val = preprocess(x_val, y_val, corpus)\n",
        "x_test, y_test = preprocess(x_test, y_test, corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary is 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "-uD8_C4p30Fd",
        "outputId": "929ec9ec-4843-47fd-fe33-52c00d636c14"
      },
      "source": [
        "#analysis of word count in reviews \n",
        "rev_len = [len(i) for i in x_train]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    40000.000000\n",
              "mean        69.295200\n",
              "std         48.098095\n",
              "min          0.000000\n",
              "25%         39.000000\n",
              "50%         54.000000\n",
              "75%         85.000000\n",
              "max        653.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_dYbS43QS9G"
      },
      "source": [
        "From the above data, it can be seen that maximum length of review is 653 and 75% of the reviews have length less than 85. Furthermore length of reviews greater than 300 is not significant (from the graph) so will take maximum length of reviews to be 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x67A2Pd6TPgt"
      },
      "source": [
        "def padding_(sentences, seq_len):\n",
        "  \"\"\" to tackle variable length of sequences: this function prepads reviews with 0 for reviews whose length is less than seq_len, and truncates reviews with length greater than\n",
        "  seq_len by removing words after seq_len in review\"\"\"\n",
        "\n",
        "  features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "  for ii, review in enumerate(sentences):\n",
        "    diff = seq_len - len(review)\n",
        "\n",
        "    if diff > 0:\n",
        "      features[ii,diff:] = np.array(review)\n",
        "\n",
        "    else:\n",
        "      features[ii] = np.array(review[:seq_len])\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6TCxJZG397Q"
      },
      "source": [
        "# maximum review length (300)\n",
        "x_train_pad = padding_(x_train,300)\n",
        "x_val_pad = padding_(x_val,300)\n",
        "x_test_pad = padding_(x_test, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thyUJtA63HXc",
        "outputId": "c19d9928-4e07-4c21-ced7-f5a395611a45"
      },
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# use GPU if available\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLOVBmSs4CIJ"
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# dataloaders\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkjELbEc4DQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ddf6b23-67c6-4c3b-b27e-8107f309c332"
      },
      "source": [
        "# one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) \n",
        "print('Sample input: \\n', sample_x)\n",
        "print('Sample output: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([50, 300])\n",
            "Sample input: \n",
            " tensor([[  0,   0,   0,  ..., 572,   1,   1],\n",
            "        [  0,   0,   0,  ...,  38, 457,  87],\n",
            "        [  0,   0,   0,  ..., 168, 841, 253],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ..., 171,   5, 225],\n",
            "        [  0,   0,   0,  ...,   9, 446,   2],\n",
            "        [  0,   0,   0,  ..., 917, 179,  95]])\n",
            "Sample output: \n",
            " tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/proxy/JeOqyqJyqifJLvX8Wet6hHNIOZ_wui2xfIkYJsK6fuE13cNJlZxxqe6vZcEe__kIagkOFolHZtyZ150yayUHpBkekTAwdMUg1MNrmVFbd1eumvusUs1zLALKLB5AA3fK\" alt=\"RNN LSTM GRU\" width=\"700\" height=\"250\">\n",
        "\n",
        "Image Credit- [Article](http://dprogrammer.org/rnn-lstm-gru)"
      ],
      "metadata": {
        "id": "NzGSXvh9s5SD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VAacQLI99WN"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2m48o21-DhM"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,output_dim):\n",
        "      super(RNN,self).__init__()\n",
        "\n",
        "      self.output_dim = output_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "\n",
        "      self.no_layers = no_layers\n",
        "      self.vocab_size = vocab_size\n",
        "\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "      self.rnn = nn.RNN(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                          num_layers=no_layers, batch_first=True)\n",
        "      \n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "      self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "      batch_size = x.size(0)\n",
        "      embeds = self.embedding(x)  \n",
        "      rnn_out, hidden = self.rnn(embeds, hidden)\n",
        "      rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
        "      out = self.dropout(rnn_out)\n",
        "      out = self.fc(out)\n",
        "      sig_out = self.sig(out)\n",
        "      sig_out = sig_out.view(batch_size, -1)\n",
        "      sig_out = sig_out[:, -1] \n",
        "      return sig_out, hidden\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        return h0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcDcAGJbMzwC"
      },
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(corpus) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYgl8jLw4W8i",
        "outputId": "f997e84e-2bbe-4586-e409-fe342f1dae0b"
      },
      "source": [
        "model = RNN(no_layers,vocab_size,hidden_dim,embedding_dim,output_dim)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(1001, 64)\n",
            "  (rnn): RNN(64, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6vqxs56M-hH"
      },
      "source": [
        "lr=0.001 #learning rate\n",
        "criterion = nn.BCELoss() #Binary Cross Entropy loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
        "\n",
        "def acc(pred,label):\n",
        "  \"\"\" function to calculate accuracy \"\"\"\n",
        "  pred = torch.round(pred.squeeze())\n",
        "  return torch.sum(pred == label.squeeze()).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afwoFOQfE2E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c90147-d2a0-4254-e638-136589a62631"
      },
      "source": [
        "patience_early_stopping = 3  #training will stop if model performance does not improve for these many consecutive epochs\n",
        "cnt = 0 #counter for checking patience level\n",
        "prev_epoch_acc = 0.0 #initializing prev test accuracy for early stopping condition\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', factor = 0.2, patience = 1) \n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    \n",
        "    for inputs, labels in train_loader: #training in batches\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)   \n",
        "        h = model.init_hidden(batch_size) \n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "        \n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "     \n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        \n",
        "        optimizer.step()\n",
        " \n",
        "    \n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    \n",
        "    for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            accuracy = acc(output,labels)\n",
        "            val_acc += accuracy\n",
        "            \n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    scheduler.step(epoch_val_acc)\n",
        "\n",
        "    if epoch_val_acc > prev_epoch_acc: #check if val accuracy for current epoch has improved compared to previous epoch\n",
        "      cnt = 0                    #f accuracy improves reset counter to 0\n",
        "    else:                        #otherwise increment current counter\n",
        "      cnt += 1\n",
        "    prev_epoch_acc = epoch_val_acc\n",
        "\n",
        "    print(f'Epoch {epoch+1}') \n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "\n",
        "    if cnt == patience_early_stopping:\n",
        "      print(f\"early stopping as test accuracy did not improve for {patience_early_stopping} consecutive epochs\")\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_accuracy : 52.980000000000004 val_accuracy : 50.160000000000004\n",
            "Epoch 2\n",
            "train_accuracy : 53.63250000000001 val_accuracy : 50.18\n",
            "Epoch 3\n",
            "train_accuracy : 62.480000000000004 val_accuracy : 75.12\n",
            "Epoch 4\n",
            "train_accuracy : 70.565 val_accuracy : 75.14\n",
            "Epoch 5\n",
            "train_accuracy : 71.33500000000001 val_accuracy : 65.52\n",
            "Epoch 6\n",
            "train_accuracy : 73.4475 val_accuracy : 69.26\n",
            "Epoch 7\n",
            "train_accuracy : 76.96 val_accuracy : 78.58000000000001\n",
            "Epoch 8\n",
            "train_accuracy : 74.6225 val_accuracy : 78.16\n",
            "Epoch 9\n",
            "train_accuracy : 76.14999999999999 val_accuracy : 66.22\n",
            "Epoch 10\n",
            "train_accuracy : 67.0575 val_accuracy : 70.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "        word_seq = np.array([corpus[preprocess_string(word)] for word in text.split() \n",
        "                         if preprocess_string(word) in corpus.keys()])\n",
        "        word_seq = np.expand_dims(word_seq,axis=0)\n",
        "        pad =  torch.from_numpy(padding_(word_seq,300))\n",
        "        input = pad.to(device)\n",
        "        batch_size = 1\n",
        "        h = model.init_hidden(batch_size)\n",
        "        #h = tuple([each.data for each in h])\n",
        "        output, h = model(input, h)\n",
        "        return(output.item())"
      ],
      "metadata": {
        "id": "YflDgiaeXAkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 30\n",
        "print(df_test['text'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df_test[\"label\"][index]}')\n",
        "print('='*70)\n",
        "prob = predict_text(df_test['text'][index])\n",
        "status = \"positive\" if prob > 0.5 else \"negative\"\n",
        "prob = (1 - prob) if status == \"negative\" else prob\n",
        "print(f'Predicted sentiment is {status} with a probability of {prob}')"
      ],
      "metadata": {
        "id": "OSsI3uNYXPBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7662d4-8907-4b29-d1af-cca6ee0e1335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie is good for entertainment purposes, but it is not historically reliable. If you are looking for a movie and thinking to yourself `Oh I want to learn more about Custer's life and his last stand', do not rent `They Died with Their Boots On'. But, if you would like to watch a movie for the enjoyment of an older western film, with a little bit of romance and just for a good story, this is a fun movie to watch.<br /><br />The story starts out with Custer's (Errol Flynn) first day at West Point. Everyone loves his charming personality which allows him to get away with most everything. The movie follows his career from West Point and his many battles, including his battle in the Civil War. The movie ends with his last stand at Little Big Horn. In between the battle scenes, he finds love and marriage with Libby (Olivia De Havilland).<br /><br />Errol Flynn portrays the arrogant, but suave George Armstrong Custer well. Olivia De Havilland plays the cute, sweet Libby very well, especially in the flirting scene that Custer and Libby first meet. Their chemistry on screen made you believe in their romance. The acting in general was impressive, especially the comedic role ( although stereotypical) of Callie played by Hattie McDaniel. Her character will definitely make you laugh.<br /><br />The heroic war music brought out the excitement of the battle scenes. The beautiful costumes set the tone of the era. The script, at times, was corny, although the movie was still enjoyable to watch. The director's portrayal of Custer was as a hero and history shows this is debatable. Some will watch this movie and see Custer as a hero. Others will watch this movie and learn hate him.<br /><br />I give it a thumbs up for this 1942 western film.\n",
            "======================================================================\n",
            "Actual sentiment is  : 1\n",
            "======================================================================\n",
            "Predicted sentiment is negative with a probability of 0.5339558720588684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFWjVHHIo8e"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hESVdw8bI53r"
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim):\n",
        "      super(GRU,self).__init__()\n",
        "\n",
        "      self.output_dim = output_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.no_layers = no_layers\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "      self.gru = nn.GRU(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                          num_layers=no_layers, batch_first=True)\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "      self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "      batch_size = x.size(0)\n",
        "      embeds = self.embedding(x)  \n",
        "      gru_out, hidden = self.gru(embeds, hidden)\n",
        "      gru_out = gru_out.contiguous().view(-1, self.hidden_dim) \n",
        "\n",
        "      out = self.dropout(gru_out)\n",
        "      out = self.fc(out)\n",
        "      sig_out = self.sig(out)\n",
        "      sig_out = sig_out.view(batch_size, -1)\n",
        "      sig_out = sig_out[:, -1] \n",
        "      return sig_out, hidden\n",
        "        \n",
        "        \n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "      ''' Initializes hidden state '''\n",
        "      h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "      return h0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJJxXfb9I53t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b405a5-e846-4dc7-a2c5-029c0910134b"
      },
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(corpus) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "model = GRU(no_layers,vocab_size,hidden_dim,embedding_dim)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU(\n",
            "  (embedding): Embedding(1001, 64)\n",
            "  (gru): GRU(64, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCBUpSx_NguS"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to calculate accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urPdufikI53w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af70b0f-6ebd-4eee-955f-da6a439c08ce"
      },
      "source": [
        "patience_early_stopping = 3  #training will stop if model performance does not improve for these many consecutive epochs\n",
        "cnt = 0 #counter for checking patience level\n",
        "prev_epoch_acc = 0.0 #initializing prev test accuracy for early stopping condition\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', factor = 0.2, patience = 1) \n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)   \n",
        "        h = model.init_hidden(batch_size)\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "        \n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "     \n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        \n",
        "        optimizer.step()\n",
        " \n",
        "    \n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    \n",
        "    for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            accuracy = acc(output,labels)\n",
        "            val_acc += accuracy\n",
        "            \n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    scheduler.step(epoch_val_acc)\n",
        "\n",
        "    if epoch_val_acc > prev_epoch_acc: #check if val accuracy for current epoch has improved compared to previous epoch\n",
        "      cnt = 0                    #f accuracy improves reset counter to 0\n",
        "    else:                        #otherwise increment current counter\n",
        "      cnt += 1\n",
        "    prev_epoch_acc = epoch_val_acc\n",
        "\n",
        "    print(f'Epoch {epoch+1}') \n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "\n",
        "    if cnt == patience_early_stopping:\n",
        "      print(f\"early stopping as test accuracy did not improve for {patience_early_stopping} consecutive epochs\")\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_accuracy : 77.125 val_accuracy : 83.08\n",
            "Epoch 2\n",
            "train_accuracy : 85.845 val_accuracy : 85.32\n",
            "Epoch 3\n",
            "train_accuracy : 87.9075 val_accuracy : 85.66\n",
            "Epoch 4\n",
            "train_accuracy : 89.92750000000001 val_accuracy : 85.64\n",
            "Epoch 5\n",
            "train_accuracy : 93.13 val_accuracy : 84.89999999999999\n",
            "Epoch 6\n",
            "train_accuracy : 98.02499999999999 val_accuracy : 84.78\n",
            "early stopping as test accuracy did not improve for 3 consecutive epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 30\n",
        "print(df_test['text'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df_test[\"label\"][index]}')\n",
        "print('='*70)\n",
        "prob = predict_text(df_test['text'][index])\n",
        "status = \"positive\" if prob > 0.5 else \"negative\"\n",
        "prob = (1 - prob) if status == \"negative\" else prob\n",
        "print(f'Predicted sentiment is {status} with a probability of {prob}')"
      ],
      "metadata": {
        "id": "3XdlPe6lc-Q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b9cba8-edf2-4b71-fd3c-ff7dca0a92cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie is good for entertainment purposes, but it is not historically reliable. If you are looking for a movie and thinking to yourself `Oh I want to learn more about Custer's life and his last stand', do not rent `They Died with Their Boots On'. But, if you would like to watch a movie for the enjoyment of an older western film, with a little bit of romance and just for a good story, this is a fun movie to watch.<br /><br />The story starts out with Custer's (Errol Flynn) first day at West Point. Everyone loves his charming personality which allows him to get away with most everything. The movie follows his career from West Point and his many battles, including his battle in the Civil War. The movie ends with his last stand at Little Big Horn. In between the battle scenes, he finds love and marriage with Libby (Olivia De Havilland).<br /><br />Errol Flynn portrays the arrogant, but suave George Armstrong Custer well. Olivia De Havilland plays the cute, sweet Libby very well, especially in the flirting scene that Custer and Libby first meet. Their chemistry on screen made you believe in their romance. The acting in general was impressive, especially the comedic role ( although stereotypical) of Callie played by Hattie McDaniel. Her character will definitely make you laugh.<br /><br />The heroic war music brought out the excitement of the battle scenes. The beautiful costumes set the tone of the era. The script, at times, was corny, although the movie was still enjoyable to watch. The director's portrayal of Custer was as a hero and history shows this is debatable. Some will watch this movie and see Custer as a hero. Others will watch this movie and learn hate him.<br /><br />I give it a thumbs up for this 1942 western film.\n",
            "======================================================================\n",
            "Actual sentiment is  : 1\n",
            "======================================================================\n",
            "Predicted sentiment is positive with a probability of 0.9998726844787598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcfUICOtBmSe"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txh-2ZL04PV0"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim):\n",
        "      super(LSTM,self).__init__()\n",
        "\n",
        "      self.output_dim = output_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "\n",
        "      self.no_layers = no_layers\n",
        "      self.vocab_size = vocab_size\n",
        "  \n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim) #embedding layer\n",
        "      \n",
        "      self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                          num_layers=no_layers, batch_first=True)  #lstm layer\n",
        "      \n",
        "      self.dropout = nn.Dropout(0.3)  # dropout layer\n",
        "      self.fc = nn.Linear(self.hidden_dim, output_dim) #fully connected layer\n",
        "      self.sig = nn.Sigmoid() #sigmoid activation \n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "      batch_size = x.size(0)\n",
        "      embeds = self.embedding(x)  \n",
        "      lstm_out, hidden = self.lstm(embeds, hidden)  \n",
        "      lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "      out = self.dropout(lstm_out)\n",
        "      out = self.fc(out)\n",
        "      sig_out = self.sig(out)\n",
        "      sig_out = sig_out.view(batch_size, -1)\n",
        "      sig_out = sig_out[:, -1] \n",
        "      return sig_out, hidden\n",
        "        \n",
        "        \n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "      ''' Initializes hidden state and cell state for LSTM '''\n",
        "      h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "      c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "      hidden = (h0,c0)\n",
        "      return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDeYqjr_FMHk"
      },
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(corpus) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3s5V6XESpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7bbc5f-fdbd-4e75-c437-d5c10fea2d3f"
      },
      "source": [
        "model = LSTM(no_layers,vocab_size,hidden_dim,embedding_dim)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(1001, 64)\n",
            "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u21-MDf4aoi"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to calculate accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r0VecTD4w1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fea1302-d980-4ead-c04f-76104f2492d2"
      },
      "source": [
        "patience_early_stopping = 3  #training will stop if model performance does not improve for these many consecutive epochs\n",
        "cnt = 0 #counter for checking patience level\n",
        "prev_epoch_acc = 0.0 #initializing prev test accuracy for early stopping condition\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', factor = 0.2, patience = 1) \n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_acc = 0.0\n",
        "  model.train()\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)  \n",
        "    h = model.init_hidden(batch_size)\n",
        "    model.zero_grad()\n",
        "    output,h = model(inputs,h)\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "    accuracy = acc(output,labels)\n",
        "    train_acc += accuracy\n",
        "    optimizer.step()\n",
        "  \n",
        "  val_acc = 0.0\n",
        "  model.eval()\n",
        "  for inputs, labels in valid_loader:\n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, val_h = model(inputs, val_h)\n",
        "    accuracy = acc(output,labels)\n",
        "    val_acc += accuracy\n",
        "          \n",
        "  epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "  epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "  scheduler.step(epoch_val_acc)\n",
        "\n",
        "  if epoch_val_acc > prev_epoch_acc: #check if val accuracy for current epoch has improved compared to previous epoch\n",
        "    cnt = 0                    #f accuracy improves reset counter to 0\n",
        "  else:                        #otherwise increment current counter\n",
        "    cnt += 1\n",
        "  prev_epoch_acc = epoch_val_acc\n",
        "\n",
        "  print(f'Epoch {epoch+1}') \n",
        "  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "  if cnt == patience_early_stopping:\n",
        "    print(f\"early stopping as test accuracy did not improve for {patience_early_stopping} consecutive epochs\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_accuracy : 74.3825 val_accuracy : 80.17999999999999\n",
            "Epoch 2\n",
            "train_accuracy : 83.9875 val_accuracy : 85.68\n",
            "Epoch 3\n",
            "train_accuracy : 86.575 val_accuracy : 85.9\n",
            "Epoch 4\n",
            "train_accuracy : 87.9575 val_accuracy : 85.46000000000001\n",
            "Epoch 5\n",
            "train_accuracy : 89.64750000000001 val_accuracy : 86.26\n",
            "Epoch 6\n",
            "train_accuracy : 91.8325 val_accuracy : 84.36\n",
            "Epoch 7\n",
            "train_accuracy : 94.6375 val_accuracy : 85.16\n",
            "Epoch 8\n",
            "train_accuracy : 98.3175 val_accuracy : 85.14\n",
            "Epoch 9\n",
            "train_accuracy : 99.4225 val_accuracy : 85.16\n",
            "Epoch 10\n",
            "train_accuracy : 99.815 val_accuracy : 85.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 30\n",
        "print(df_test['text'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df_test[\"label\"][index]}')\n",
        "print('='*70)\n",
        "prob = predict_text(df_test['text'][index])\n",
        "status = \"positive\" if prob > 0.5 else \"negative\"\n",
        "prob = (1 - prob) if status == \"negative\" else prob\n",
        "print(f'Predicted sentiment is {status} with a probability of {prob}')"
      ],
      "metadata": {
        "id": "YAm7L0UTdD2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b842c26-8aa8-4898-a1d3-9c3c9b9175ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This movie is good for entertainment purposes, but it is not historically reliable. If you are looking for a movie and thinking to yourself `Oh I want to learn more about Custer's life and his last stand', do not rent `They Died with Their Boots On'. But, if you would like to watch a movie for the enjoyment of an older western film, with a little bit of romance and just for a good story, this is a fun movie to watch.<br /><br />The story starts out with Custer's (Errol Flynn) first day at West Point. Everyone loves his charming personality which allows him to get away with most everything. The movie follows his career from West Point and his many battles, including his battle in the Civil War. The movie ends with his last stand at Little Big Horn. In between the battle scenes, he finds love and marriage with Libby (Olivia De Havilland).<br /><br />Errol Flynn portrays the arrogant, but suave George Armstrong Custer well. Olivia De Havilland plays the cute, sweet Libby very well, especially in the flirting scene that Custer and Libby first meet. Their chemistry on screen made you believe in their romance. The acting in general was impressive, especially the comedic role ( although stereotypical) of Callie played by Hattie McDaniel. Her character will definitely make you laugh.<br /><br />The heroic war music brought out the excitement of the battle scenes. The beautiful costumes set the tone of the era. The script, at times, was corny, although the movie was still enjoyable to watch. The director's portrayal of Custer was as a hero and history shows this is debatable. Some will watch this movie and see Custer as a hero. Others will watch this movie and learn hate him.<br /><br />I give it a thumbs up for this 1942 western film.\n",
            "======================================================================\n",
            "Actual sentiment is  : 1\n",
            "======================================================================\n",
            "Predicted sentiment is positive with a probability of 0.9999948740005493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t8d4KyvXVHFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}